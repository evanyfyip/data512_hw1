{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to pull the Academy Awards Movie views data from wikimedia using the pageviews api. The first step is to load the python packages that I will need for data acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, urllib.parse\n",
    "\n",
    "import requests\n",
    "from collections import OrderedDict\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Code\n",
    "The code below is from Dr. David McDonald. I will be using it to access the pageviews API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The REST API 'pageviews' URL - this is the common URL/endpoint for all 'pageviews' API requests\n",
    "API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n",
    "\n",
    "# This is a parameterized string that specifies what kind of pageviews request we are going to make\n",
    "# In this case it will be a 'per-article' based request. The string is a format string so that we can\n",
    "# replace each parameter with an appropriate value before making the request\n",
    "API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
    "\n",
    "# The Pageviews API asks that we not exceed 100 requests per second, we add a small delay to each request\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making a request to the Wikimedia API they ask that you include your email address which will allow them\n",
    "# to contact you if something happens - such as - your code exceeding rate limits - or some other error \n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<uwnetid@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = [ 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\n",
    "\n",
    "# This template is used to map parameter values into the API_REQUST_PER_ARTICLE_PARAMS portion of an API request. The dictionary has a\n",
    "# field/key for each of the required parameters. In the example, below, we only vary the article name, so the majority of the fields\n",
    "# can stay constant for each request. Of course, these values *could* be changed if necessary.\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"desktop\",      # this should be changed for the different access types\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015010100\",   # start and end dates need to be set\n",
    "    \"end\":         \"2023040100\"    # this is likely the wrong end date\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first adjust the values of various parameters to Dr. McDonald's function. Three changed need to be made. First I will add my personal email and contact information to the request headers parameter. Then I will adjust the parameters of the pageviews api so that we are only looking at pageview data between July 1, 2015 to September 30, 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the given example code\n",
    "REQUEST_HEADERS['User-Agent'] = 'eyfy@uw.edu, University of Washington, MSDS DATA 512 - AUTUMN 2023'\n",
    "\n",
    "# Setting the start and end date to July 1, 2015 to September 30, 2023\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE['start'] = 20150701\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE['end'] = 20230930"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specifications\n",
    "The goal is to extract the monthly **user** pageview requests from July 1, 2015 to September 30, 2023. We will create three separate datasets and store them as three different JSON files based upon three different access modes.\n",
    "1. Monthly mobile access\n",
    "    - academy_monthly_mobile_201507-202309.json\n",
    "2. Monthly desktop access\n",
    "    - academy_monthly_desktop_201507-202309.json\n",
    "3. Monthly cumulative\n",
    "    - academy_monthly_cumulative_201507-202309.json\n",
    "\n",
    "\n",
    "Additional Notes:\n",
    "1. Mobile access requires two separate requests `mobile-web` and `mobile-app` access keys\n",
    "2. We are only interested in monthly activity ie `granularity = monthly`\n",
    "3. JSON file format: \n",
    "    - Ordered using article titles as a key for the resulting time series data\n",
    "    - remove the `access` field\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the new parameters into the function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the `request_pageviews_per_article` method below. By instantiating the variables above we are able to feed in the default parameters for the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS from Dr. McDonald\n",
    "#\n",
    "\n",
    "def request_pageviews_per_article(article_title = None, \n",
    "                                  endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                  endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                  request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                  headers = REQUEST_HEADERS):\n",
    "\n",
    "    # article title can be as a parameter to the call or in the request_template\n",
    "    if article_title:\n",
    "        request_template['article'] = article_title\n",
    "\n",
    "    if not request_template['article']:\n",
    "        raise Exception(\"Must supply an article title to make a pageviews request.\")\n",
    "\n",
    "    # Titles are supposed to have spaces replaced with \"_\" and be URL encoded\n",
    "    article_title_encoded = urllib.parse.quote(request_template['article'].replace(' ','_'), safe='')\n",
    "    request_template['article'] = article_title_encoded\n",
    "    \n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our methods are defined we can begin by loading in the raw data file which is a csv file containing the names of the movies (page name) and their corresponding url. We can easily load this in using the pandas package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the raw data\n",
    "data_raw = pd.read_csv('../data_raw/thank_the_academy_AUG_2023.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will parse through each movie page and pull pageview data from the wikimedia api. Because there are multiple different access types we will need to make multiple calls to the api for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_dict = {}\n",
    "desktop_dict = {}\n",
    "cumulative_dict = {}\n",
    "\n",
    "# Iterating through the names in data_raw\n",
    "for _, item in data_raw.iterrows():\n",
    "    try:\n",
    "        # Load all access types\n",
    "        ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE['access'] = 'mobile-web'\n",
    "        mobile_web = request_pageviews_per_article(item['name'], request_template=ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE)\n",
    "        ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE['access'] = 'mobile-app'\n",
    "        mobile_app = request_pageviews_per_article(item['name'], request_template=ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE)\n",
    "        ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE['access'] = 'desktop'\n",
    "        desktop = request_pageviews_per_article(item['name'], request_template=ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE)\n",
    "\n",
    "        # Convert all to pandas dataframes\n",
    "        mobile_web_df = pd.DataFrame(mobile_web['items'])\n",
    "        mobile_app_df = pd.DataFrame(mobile_app['items'])\n",
    "        desktop_df = pd.DataFrame(desktop['items'])\n",
    "\n",
    "        # Drop the access column in all dataframes\n",
    "        mobile_web_df.drop(columns=['access'], inplace=True)\n",
    "        mobile_app_df.drop(columns=['access'], inplace=True)\n",
    "        desktop_df.drop(columns=['access'], inplace=True)\n",
    "\n",
    "\n",
    "        # Making copies of the dataframes into new dataframes\n",
    "        mobile_combine_df = mobile_web_df.copy()\n",
    "        cumulative_df = desktop_df.copy()\n",
    "        # Combine the two mobile access types (web and app)\n",
    "        # Combine mobile with desktop to get total views\n",
    "        mobile_combine_df['views'] = mobile_web_df['views'] + mobile_app_df['views']\n",
    "        cumulative_df['views'] = desktop_df['views'] + mobile_combine_df['views']\n",
    "\n",
    "        # Convert back to dictionary\n",
    "        mobile_output_dict = mobile_combine_df.to_dict(orient='records')\n",
    "        desktop_output_dict = desktop_df.to_dict(orient='records')\n",
    "        cumulative_output_dict = cumulative_df.to_dict(orient='records')\n",
    "\n",
    "        # Storing movie views for each access type\n",
    "        mobile_dict[item['name']] = mobile_output_dict\n",
    "        desktop_dict[item['name']] = desktop_output_dict\n",
    "        cumulative_dict[item['name']] = cumulative_output_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(item['name'])\n",
    "        print(\"Failed!\", e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final steps are to order the dictionary based on the Movie names using the collections OrderedDict object and then store the dictionaries as JSON files. We will store the extracted data under the `data_final` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting dictionaries by key\n",
    "ordered_mobile = OrderedDict(sorted(mobile_dict.items()))\n",
    "ordered_desktop = OrderedDict(sorted(desktop_dict.items()))\n",
    "ordered_cumulative = OrderedDict(sorted(cumulative_dict.items()))\n",
    "\n",
    "# Writing dictionaries to output files\n",
    "with open('../data_final/academy_monthly_mobile_201507-202309.json', 'w+') as f:\n",
    "    json.dump(ordered_mobile, f, indent=4)\n",
    "with open('../data_final/academy_monthly_desktop_201507-202309.json', 'w+') as f:\n",
    "    json.dump(ordered_desktop, f, indent=4)\n",
    "with open('../data_final/academy_monthly_cumulative_201507-202309.json', 'w+') as f:\n",
    "    json.dump(ordered_cumulative, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msds_sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
